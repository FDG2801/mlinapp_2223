{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a transformer layer with a single head of attention in TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is:\n",
    "attention(Q,K,V)=softmax(QK.T/sqrt(d_k))V where\n",
    "- Q, K, V are the query matrix, key matrix and value matrix\n",
    "- d_k is the query and key matrix dimension\n",
    "- d_v is the value matrix dimension\n",
    "\n",
    "The **query** is the representation for the word we want to calculate self-attention for. So since we want to get the self-attention for “fluffy”, we only consider its query, not the one of “pancakes”. As soon as we are finished calculating the self-attention for “fluffy”, we can also discard its query vector.\n",
    "\n",
    "The **key** is a representation of each word in the sequence and is used to match against the query of the word for which we currently want to calculate self-attention.\n",
    "\n",
    "The **value** is the actual representation of each word in a sequence, the representation we really care about. Multiplying the query and key gives us a score that tells us how much weight each value (and thus, its corresponding word) obtains in the self-attention vector. Note that the the value is not directly multiplied with the score, but first the scores are divided by the square root of the dk, the dimension of the key vector, and softmax is applied.\n",
    "\n",
    "How does it works:\n",
    "- calculate scalar product between query matrix and key matrix transposed\n",
    "- the result is then divided for the radical square of key and query matrix dimension (sqrt(d_k)). This is needed because the scalar product may get too big and make the learning unstable\n",
    "- the division result is then passed in a softmax to obtain attention weights, used to weight value matrix and obtain output attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source(s): https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960\n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SingleHeadTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dot = tf.keras.layers.Dot()\n",
    "        self.softmax = tf.keras.activations.softmax()\n",
    "\n",
    "    def call(self, Q, K, V):\n",
    "        enum = self.dot([Q, K])\n",
    "        den = np.sqrt(K.shape[-1])\n",
    "        output = self.softmax(enum / den)\n",
    "        output *= V\n",
    "        return output\n",
    "\n",
    "class ResidualA(tf.keras.layers.Layer):\n",
    "    def __init__(self, h_size ,use_head = False) -> None:\n",
    "      super().__init__()\n",
    "      self.query = tf.keras.layers.Dense(h_size)\n",
    "      self.key = tf.keras.layers.Dense(h_size)\n",
    "      self.value = tf.keras.layers.Dense(h_size)\n",
    "      self.pos_enc = PositionalEncoding(h_size)  # cambia la dimensione dell'encoding\n",
    "      self.t_head = None\n",
    "      self.norm = tf.keras.layers.LayerNormalization()  # usa LayerNormalization invece di Normalization\n",
    "      if use_head:\n",
    "        self.t_head = SingleHeadTransformer()\n",
    "\n",
    "    def call(self, inputs):\n",
    "      inputs = self.pos_enc(inputs)\n",
    "      if self.t_head is not None:\n",
    "        Q = self.query(inputs)\n",
    "        K = self.key(inputs)\n",
    "        V = self.value(inputs)\n",
    "        inputs += self.t_head(Q,K,V)\n",
    "      inputs = self.norm(inputs)\n",
    "      return inputs\n",
    "\n",
    "class ResidualB(tf.keras.layers.Layer):\n",
    "    def __init__(self, h_size, use_res=False):\n",
    "        super().__init__()\n",
    "        self.use_res = use_res\n",
    "        self.fc1 = tf.keras.layers.Dense(4 * h_size, activation=tf.keras.activations.relu)\n",
    "        self.fc2 = tf.keras.layers.Dense(h_size)\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        if self.use_res:\n",
    "            x = self.fc1(x)\n",
    "            x = self.fc2(x)\n",
    "            x += inputs\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, n=10000):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = inputs.shape[2]\n",
    "        P = np.zeros((seq_len, self.d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(self.d / 2)):\n",
    "                denominator = np.power(self.n, 2 * i / self.d)\n",
    "                P[k, 2 * i] = np.sin(k / denominator)\n",
    "                P[k, 2 * i + 1] = np.cos(k / denominator)\n",
    "        return inputs + P\n",
    "\n",
    "class Transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_a = ResidualA(128, True)\n",
    "        self.block_b = ResidualB(128, True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_a(inputs)\n",
    "        x = self.block_b(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SingleHeadTransformer`: defines a single head of attention in the Transformer, where query, key and value goes through a scalar dot product attention and softmax activation. This layer is used in the `ResidualA` to transform a residual of the network.\n",
    "- `ResidualA`: implements the first block type of the Transformer. It is a self-attention process followed by a FFNN. The input goes through a codification, then a self-attention is applied using the `SingleHeadTransformer` \n",
    "- `ResidualB`: implements the second block type of the Transformer. It is a FFNN with a residual. \n",
    "- `PositionalEncoding`: implement the codification of input position in the Transformer. Calculates the codification position using the formula. \n",
    "- `Transformer`: defines the entire architecture, combining the two block types. `block_a` implements the self attention; while `block_b` implements the FFNN. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
